#!/usr/bin/env python3
"""
–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ò–ó –í–°–ï–• PDF –° OCR
–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö SQLite —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —Å–∫–∞–Ω–æ–≤.
"""

import os
import re
import sqlite3
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional

# –ü—É—Ç–∏
PDF_DIR = Path(sys.argv[1]) if len(sys.argv) > 1 else Path("C:/Users/New/Desktop/–ø–¥—Ñ")
OUTPUT_DIR = Path("knowledge_base_v2/data")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

class OCRProcessor:
    """OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF"""
    
    def __init__(self):
        self.tesseract_available = self._check_tesseract()
        
    def _check_tesseract(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Tesseract"""
        try:
            import pytesseract
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é
            version = pytesseract.get_tesseract_version()
            print(f"  ‚úì Tesseract –Ω–∞–π–¥–µ–Ω: {version}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            langs = pytesseract.get_languages()
            if 'rus' in langs:
                print(f"  ‚úì –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return True
            else:
                print(f"  ‚ö† –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –î–æ—Å—Ç—É–ø–Ω—ã: {langs}")
                print(f"    –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: tesseract-ocr-rus")
                return False
        except Exception as e:
            print(f"  ‚ö† Tesseract –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
            print(f"    –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Tesseract-OCR —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º")
            print(f"    –°–º. –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é: OCR_SETUP.md")
            return False
    
    def extract_with_ocr(self, pdf_path: Path, dpi: int = 300) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏—Å–ø–æ–ª—å–∑—É—è OCR"""
        if not self.tesseract_available:
            return ""
        
        try:
            from pdf2image import convert_from_path
            import pytesseract
            
            print(f"    OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞ ({dpi} DPI)...", end=" ", flush=True)
            
            text = ""
            images = convert_from_path(pdf_path, dpi=dpi, poppler_path=r"C:\poppler\poppler-24.08.0\Library\bin")
            
            for i, image in enumerate(images, 1):
                # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º
                page_text = pytesseract.image_to_string(
                    image, 
                    lang='rus',
                    config='--psm 6'  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –æ–¥–∏–Ω –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞
                )
                text += page_text + "\n"
                print(f"{i}", end="", flush=True)
            
            print(f" ‚úì")
            return text
            
        except Exception as e:
            print(f"\n    ‚úó –û—à–∏–±–∫–∞ OCR: {e}")
            return ""

class PDFTextExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Å OCR fallback"""
    
    def __init__(self):
        self.pdf_dir = PDF_DIR
        self.total_files = 0
        self.processed = 0
        self.ocr_processed = 0
        self.errors = 0
        self.ocr = OCRProcessor()
        
    def get_all_pdfs(self) -> List[Path]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö PDF"""
        files = [f for f in self.pdf_dir.iterdir() 
                if f.suffix.lower() == '.pdf']
        return sorted(files, key=lambda x: x.name.lower())
    
    def extract_with_pypdf2(self, pdf_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É—è PyPDF2"""
        try:
            import PyPDF2
            text = ""
            with open(pdf_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page_num, page in enumerate(reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            return text
        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞ PyPDF2: {e}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = ''.join(char for char in text if char.isprintable() or char == '\n')
        
        # –£–¥–∞–ª—è–µ–º OCR-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        text = re.sub(r'[_|]{3,}', '', text)  # –õ–∏–Ω–∏–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        
        return text.strip()
    
    def extract_from_pdf(self, pdf_path: Path) -> Tuple[str, Dict]:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ PDF"""
        stats = {
            'filename': pdf_path.name,
            'pages': 0,
            'chars': 0,
            'method': 'none',
            'status': 'ok'
        }
        
        # –ü—Ä–æ–±—É–µ–º PyPDF2 —Å–Ω–∞—á–∞–ª–∞
        text = self.extract_with_pypdf2(pdf_path)
        
        if text.strip() and len(text) > 100:
            # –¢–µ–∫—Å—Ç–æ–≤—ã–π PDF - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            stats['method'] = 'text'
            text = self.clean_text(text)
            stats['chars'] = len(text)
        else:
            # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π PDF - –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR
            print(f"    (—Å–∫–∞–Ω - –∑–∞–ø—É—Å–∫–∞—é OCR...)")
            text = self.ocr.extract_with_ocr(pdf_path, dpi=300)
            
            if text.strip():
                stats['method'] = 'ocr'
                stats['chars'] = len(text)
                self.ocr_processed += 1
            else:
                stats['method'] = 'failed'
                stats['status'] = 'empty_after_ocr'
        
        return text, stats
    
    def categorize_content(self, text: str, filename: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        categories = {
            'numerology': ['—á–∏—Å–ª–æ', '—Ü–∏—Ñ—Ä–∞', '–Ω—É–º–µ—Ä–æ–ª–æ–≥', '—Ä–æ–∂–¥–µ–Ω–∏—è', '–ø—É—Ç—å –∂–∏–∑–Ω–∏', '—Å—É–¥—å–±–∞'],
            'calculations': ['—Ä–∞—Å—á–µ—Ç', '—Ä–∞—Å—á—ë—Ç', '—Ñ–æ—Ä–º—É–ª–∞', '–≤—ã—á–∏—Å–ª', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ç–æ—á–∫–∞'],
            'practices': ['–ø—Ä–∞–∫—Ç–∏–∫–∞', '–º–µ–¥–∏—Ç–∞—Ü–∏—è', '–º–æ–ª–∏—Ç–≤–∞', '—Ç–µ—Ö–Ω–∏–∫–∞', '—Ä–∏—Ç—É–∞–ª', '–ø–æ–∫–ª–æ–Ω'],
            'diagnostics': ['–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞', '–∞–Ω–∞–ª–∏–∑', '–∫–∞—Ä—Ç–∞', '–≥–µ–Ω–æ–≥—Ä–∞–º–º–∞', '–ø—Ä–æ–∫–ª—è—Ç–∏'],
            'ancestral': ['—Ä–æ–¥', '—Ä–æ–¥–æ–≤–æ–π', '–ø—Ä–µ–¥–∫–∏', '—Å–µ–º—å—è', '–ø–æ–∫–æ–ª–µ–Ω', '—Ä–æ–¥–∏—Ç–µ–ª—å'],
            'financial': ['–¥–µ–Ω—å–≥–∏', '—Ñ–∏–Ω–∞–Ω—Å', '–±–∏–∑–Ω–µ—Å', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–¥–æ—Ö–æ–¥', '–∫–∞–Ω–∞–ª'],
            'health': ['–∑–¥–æ—Ä–æ–≤—å–µ', '–±–æ–ª–µ–∑–Ω—å', '—Ç—Ä–∞–≤–º–∞', '–∏—Å—Ü–µ–ª–µ–Ω–∏–µ', '–ø—Å–∏—Ö–æ–ª–æ–≥'],
            'relationships': ['–æ—Ç–Ω–æ—à–µ–Ω–∏—è', '–±—Ä–∞–∫', '–ø–∞—Ä—Ç–Ω–µ—Ä', '—Å–µ–º—å—è', '–±–ª–∏–∑–Ω–µ—Ü'],
            'energy': ['—á–∞–∫—Ä', '—ç–Ω–µ—Ä–≥', '–∫–∞–Ω–∞–ª', '–ø–æ—Ç–æ–∫', '–≤–∏–±—Ä–∞—Ü'],
            'psychology': ['–ø—Å–∏—Ö–æ–ª–æ–≥', '—Ç—Ä–∞–≤–º–∞', '–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π', '—Ä–µ–±–µ–Ω–æ–∫', '—Å—Ü–µ–Ω–∞—Ä–∏–π']
        }
        
        detected = []
        for cat, keywords in categories.items():
            score = sum(2 for kw in keywords if kw in text_lower)
            score += sum(3 for kw in keywords if kw in filename_lower)
            if score > 2:
                detected.append((cat, score))
        
        detected.sort(key=lambda x: x[1], reverse=True)
        return [cat for cat, _ in detected[:3]]
    
    def determine_type(self, filename: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        fname = filename.lower()
        
        type_patterns = {
            'calculator': ['—Ä–∞—Å—á–µ—Ç', '—Ä–∞—Å—á—ë—Ç', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä'],
            'practice': ['–ø—Ä–∞–∫—Ç–∏–∫–∞', '–º–µ–¥–∏—Ç–∞—Ü–∏—è', '–º–æ–ª–∏—Ç–≤–∞'],
            'algorithm': ['–∞–ª–≥–æ—Ä–∏—Ç–º', '—Å—Ö–µ–º–∞', '–≤—ã–±–æ—Ä', '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞'],
            'template': ['–∫–∞—Ä—Ç–∞', '–≥–µ–Ω–æ–≥—Ä–∞–º–º–∞', '–º–∞—Ç—Ä–∏—Ü–∞'],
            'reference': ['—Å–±–æ—Ä–Ω–∏–∫', '–∫–Ω–∏–≥–∞', '–∑–Ω–∞—á–µ–Ω–∏—è'],
            'guide': ['–∫–∞–∫', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è']
        }
        
        for doc_type, patterns in type_patterns.items():
            if any(p in fname for p in patterns):
                return doc_type
        return 'reference'

class SQLiteBuilder:
    """–°–æ–∑–¥–∞–Ω–∏–µ SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.db_path = OUTPUT_DIR / "knowledge_base.db"
        self.conn = None
        self.cursor = None
        
    def connect(self):
        """–ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ"""
        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.row_factory = sqlite3.Row
        self.cursor = self.conn.cursor()
        
    def create_tables(self):
        """–°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—ã"""
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT NOT NULL,
                title TEXT,
                doc_type TEXT,
                categories TEXT,
                content TEXT,
                content_length INTEGER,
                extraction_method TEXT,
                extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.cursor.execute('''
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                filename,
                title,
                content,
                content='documents',
                content_rowid='id'
            )
        ''')
        
        # –¢—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ FTS
        self.cursor.execute('''
            CREATE TRIGGER IF NOT EXISTS documents_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, filename, title, content)
                VALUES (new.id, new.filename, new.title, new.content);
            END
        ''')
        
        self.cursor.execute('''
            CREATE TRIGGER IF NOT EXISTS documents_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, filename, title, content)
                VALUES ('delete', old.id, old.filename, old.title, old.content);
            END
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –∏–Ω–¥–µ–∫—Å–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS category_index (
                category TEXT,
                doc_id INTEGER,
                FOREIGN KEY (doc_id) REFERENCES documents(id)
            )
        ''')
        
        self.conn.commit()
        
    def insert_document(self, filename: str, title: str, doc_type: str, 
                       categories: List[str], content: str, content_length: int,
                       extraction_method: str) -> int:
        """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É"""
        self.cursor.execute('''
            INSERT INTO documents 
            (filename, title, doc_type, categories, content, content_length, extraction_method)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (filename, title, doc_type, json.dumps(categories), content, content_length, extraction_method))
        
        doc_id = self.cursor.lastrowid
        
        # –î–æ–±–∞–≤–∏—Ç—å –≤ –∏–Ω–¥–µ–∫—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for cat in categories:
            self.cursor.execute(
                'INSERT INTO category_index (category, doc_id) VALUES (?, ?)',
                (cat, doc_id)
            )
        
        self.conn.commit()
        return doc_id
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        self.cursor.execute('SELECT COUNT(*), SUM(content_length) FROM documents')
        total, total_chars = self.cursor.fetchone()
        
        self.cursor.execute('''
            SELECT doc_type, COUNT(*) FROM documents GROUP BY doc_type
        ''')
        by_type = dict(self.cursor.fetchall())
        
        self.cursor.execute('''
            SELECT extraction_method, COUNT(*) FROM documents GROUP BY extraction_method
        ''')
        by_method = dict(self.cursor.fetchall())
        
        self.cursor.execute('''
            SELECT category, COUNT(DISTINCT doc_id) 
            FROM category_index 
            GROUP BY category
        ''')
        by_category = dict(self.cursor.fetchall())
        
        return {
            'total_documents': total,
            'total_chars': total_chars,
            'total_mb': round(total_chars / (1024 * 1024), 2),
            'by_type': by_type,
            'by_method': by_method,
            'by_category': by_category
        }
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        if self.conn:
            self.conn.close()

class HybridKnowledgeBuilder:
    """–ì–ª–∞–≤–Ω—ã–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å –≥–∏–±—Ä–∏–¥–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self):
        self.extractor = PDFTextExtractor()
        self.db = SQLiteBuilder()
        self.stats = {
            'processed': 0,
            'errors': 0,
            'empty': 0,
            'total_chars': 0
        }
        
    def build(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–ª–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        print("=" * 80)
        print("–°–û–ó–î–ê–ù–ò–ï –ü–û–õ–ù–û–¢–ï–ö–°–¢–û–í–û–ô –ë–ê–ó–´ –î–ê–ù–ù–´–• –° OCR")
        print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤—Å–µ—Ö PDF")
        print("=" * 80)
        print()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º OCR
        print("–ü—Ä–æ–≤–µ—Ä–∫–∞ OCR:")
        if self.extractor.ocr.tesseract_available:
            print("  ‚úì Tesseract —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º –¥–æ—Å—Ç—É–ø–µ–Ω")
            print("  ‚úì OCR –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö PDF")
        else:
            print("  ‚ö† Tesseract –Ω–µ –Ω–∞–π–¥–µ–Ω - —Å–º. OCR_SETUP.md")
            print("  ‚ö† –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ PDF –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")
        print()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        pdfs = self.extractor.get_all_pdfs()
        total = len(pdfs)
        print(f"–ù–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤: {total}")
        print(f"–ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä: ~210 MB")
        print()
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
        print("–°–æ–∑–¥–∞–Ω–∏–µ SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        self.db.connect()
        self.db.create_tables()
        print("‚úì –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞")
        print()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π PDF
        print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF...")
        print("-" * 80)
        
        for i, pdf_path in enumerate(pdfs, 1):
            print(f"[{i:2d}/{total}] {pdf_path.name[:45]}...", end=" ", flush=True)
            
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç (—Å OCR –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
                text, file_stats = self.extractor.extract_from_pdf(pdf_path)
                
                if not text.strip():
                    self.stats['empty'] += 1
                    print(f"‚ö† (–ø—É—Å—Ç–æ–π)")
                    continue
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º
                categories = self.extractor.categorize_content(text, pdf_path.name)
                doc_type = self.extractor.determine_type(pdf_path.name)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É
                doc_id = self.db.insert_document(
                    filename=pdf_path.name,
                    title=pdf_path.stem.replace('_', ' '),
                    doc_type=doc_type,
                    categories=categories,
                    content=text,
                    content_length=len(text),
                    extraction_method=file_stats['method']
                )
                
                self.stats['processed'] += 1
                self.stats['total_chars'] += len(text)
                method_icon = "üìù" if file_stats['method'] == 'text' else "üîç"
                print(f"{method_icon} {len(text):,} —Å–∏–º–≤–æ–ª–æ–≤")
                
            except Exception as e:
                self.stats['errors'] += 1
                print(f"‚úó –û—à–∏–±–∫–∞: {e}")
        
        print()
        print("-" * 80)
        print("–ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò:")
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['processed']}")
        print(f"  –ü—É—Å—Ç—ã—Ö: {self.stats['empty']}")
        print(f"  –û—à–∏–±–æ–∫: {self.stats['errors']}")
        print(f"  OCR –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.extractor.ocr_processed}")
        print(f"  –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {self.stats['total_chars']:,}")
        print()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã
        db_stats = self.db.get_stats()
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•:")
        print(f"  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {db_stats['total_documents']}")
        print(f"  –û–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {db_stats['total_mb']:.2f} MB")
        print(f"  –ü–æ –º–µ—Ç–æ–¥–∞–º: {db_stats['by_method']}")
        print(f"  –ü–æ —Ç–∏–ø–∞–º: {db_stats['by_type']}")
        print()
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Ç–µ—Ä-–∏–Ω–¥–µ–∫—Å
        self._create_master_index(db_stats)
        
        self.db.close()
        print("‚úÖ –ì–æ—Ç–æ–≤–æ! –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞.")
        
    def _create_master_index(self, db_stats: Dict):
        """–°–æ–∑–¥–∞—Ç—å –º–∞—Å—Ç–µ—Ä-–∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        master_index = {
            "version": "2.1",
            "created": datetime.now().isoformat(),
            "ocr_enabled": self.extractor.ocr.tesseract_available,
            "structure": {
                "lightweight": {
                    "description": "–õ—ë–≥–∫–∏–µ JSON –¥–ª—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞",
                    "files": [
                        "formulas.json (15 —Ñ–æ—Ä–º—É–ª)",
                        "practices.json (8 –ø—Ä–∞–∫—Ç–∏–∫)",
                        "number_meanings.json (11 –∑–Ω–∞—á–µ–Ω–∏–π)",
                        "algorithms.json (2 –∞–ª–≥–æ—Ä–∏—Ç–º–∞)"
                    ],
                    "size_kb": 41
                },
                "fulltext": {
                    "description": "–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–∞—è –±–∞–∑–∞ SQLite —Å OCR",
                    "file": "knowledge_base.db",
                    "documents": db_stats['total_documents'],
                    "size_mb": db_stats['total_mb'],
                    "extraction_methods": db_stats['by_method'],
                    "categories": list(db_stats['by_category'].keys())
                }
            }
        }
        
        with open(OUTPUT_DIR / 'master_index.json', 'w', encoding='utf-8') as f:
            json.dump(master_index, f, ensure_ascii=False, indent=2)
        
        print("‚úì –ú–∞—Å—Ç–µ—Ä-–∏–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª–µ–Ω")

def main():
    builder = HybridKnowledgeBuilder()
    builder.build()
    
    print("\n" + "=" * 80)
    print("–ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
    print("=" * 80)
    print()
    print("–°–æ–∑–¥–∞–Ω–æ:")
    print("  üìÑ Lightweight: JSON —Ñ–∞–π–ª—ã (41 KB) - –¥–ª—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞")
    print("  üóÑÔ∏è  Full-text: SQLite –±–∞–∑–∞ - –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–º PDF (—Å OCR)")
    print()
    print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
    print("  –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä ‚Üí knowledge_base.py + data/*.json")
    print("  –ü–æ–∏—Å–∫ ‚Üí SQLite knowledge_base.db")
    print()
    print("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç:")
    print("  python knowledge_base.py")

if __name__ == '__main__':
    main()
